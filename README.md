# L9Trace

L9Trace is a locally run tool for monitoring the toxicity of text outputs from large language models.
It uses Detoxify for scoring and logging responses for audit.

## Features

- âœ… Real-time toxicity scoring with Detoxify
- âœ… JSON-based logging of inputs and severity
- âœ… Customizable toxicity threshold
- âœ… (Optional) Email alerting system
- Ready to migrate to AWS (Lambda, S3, CloudWatch, SNS)

## ğŸ§  Use Cases

- Testing LLM output safety
- Creating toxicity datasets from model generations
- Practicing responsible AI deployment

## ğŸ› ï¸ Installation

1. Clone the repo

```bash
git clone https://github.com/your-username/L9Trace.git
cd L9Trace
```
