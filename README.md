# L9Trace

L9Trace is a locally run tool for monitoring the toxicity of text outputs from large language models.
It uses Detoxify for scoring and logging responses for audit.

## Features

- ✅ Real-time toxicity scoring with Detoxify
- ✅ JSON-based logging of inputs and severity
- ✅ Customizable toxicity threshold
- ✅ (Optional) Email alerting system
- Ready to migrate to AWS (Lambda, S3, CloudWatch, SNS)

## 🧠 Use Cases

- Testing LLM output safety
- Creating toxicity datasets from model generations
- Practicing responsible AI deployment

## 🛠️ Installation

1. Clone the repo

```bash
git clone https://github.com/your-username/L9Trace.git
cd L9Trace
```
